%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{times}
\expandafter\ifx\csname T@LGR\endcsname\relax
\else
% LGR was declared as font encoding
  \substitutefont{LGR}{\rmdefault}{cmr}
  \substitutefont{LGR}{\sfdefault}{cmss}
  \substitutefont{LGR}{\ttdefault}{cmtt}
\fi
\expandafter\ifx\csname T@X2\endcsname\relax
  \expandafter\ifx\csname T@T2A\endcsname\relax
  \else
  % T2A was declared as font encoding
    \substitutefont{T2A}{\rmdefault}{cmr}
    \substitutefont{T2A}{\sfdefault}{cmss}
    \substitutefont{T2A}{\ttdefault}{cmtt}
  \fi
\else
% X2 was declared as font encoding
  \substitutefont{X2}{\rmdefault}{cmr}
  \substitutefont{X2}{\sfdefault}{cmss}
  \substitutefont{X2}{\ttdefault}{cmtt}
\fi


\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\usepackage{sphinxmessages}
\setcounter{tocdepth}{1}



\title{Refrigerator temperature prediction}
\date{Jun 26, 2020}
\release{}
\author{Jacek Dobrowolski}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Dokumentacja wstępna UM}
\label{\detokenize{dok_wstepna:dokumentacja-wstepna-um}}\label{\detokenize{dok_wstepna::doc}}\begin{quote}

\sphinxstylestrong{Temat:} Algorytm maszynowego uczenia się w zastosowaniu do predykcji
wartości szeregu czasowego. Zadanie polega na przewidywaniu temperatury
w chłodziarce w zależności od temperatury zewnętrznej i stanu agregatu.
\end{quote}


\section{Opis Algorytmu}
\label{\detokenize{dok_wstepna:opis-algorytmu}}
W projekcie planuje wykorzystać algorytm lasu losowego drzew
regresyjnych.


\section{Dane}
\label{\detokenize{dok_wstepna:dane}}
Najpierw należy rozdzielić dane na dane trenujące i dane testowe.
Ostatnie 10\% danych przeznaczę na testy. Reszta danych będzie podzielona
na mniejsze fragmenty na podstawie których będą tworzone poszczególne
drzewa regresyjne. Dane zawierają temperaturę wewnątrz chłodziarki,
temperaturę zewnętrzną i stan pracy agregatu z poprzedniego kwantu
czasu, oraz temperaturę wewnętrzną chłodziarki z bieżącego czasu.


\section{Tworzenie drzew regresyjnych}
\label{\detokenize{dok_wstepna:tworzenie-drzew-regresyjnych}}
Drzewo składa się z węzłów i liści oraz korzenia (pierwszego węzła).
Każdy węzeł zawiera warunek decydującym czy wybieramy prawą czy lewą
gałąź. Gałęzie zakończone są liśćmi, które reprezentują wynik algorytmu.

Tworzenie drzewa zaczynamy od korzenia. Przebieg tworzenia każdego węzła
jest jednakowy. Najpierw sprawdzamy rozmiar zbioru, jeśli jest mniejszy
niż dany parametr to węzeł staje się liściem. A jego wartość to średnia
wartości szukanej w wybranym zbiorze. Jeśli zbiór jest większy szukamy
najlepszego miejsca podziału zbioru.

Losujemy atrybut według którego podzielimy zbiór. Następnie sortujemy
według tego atrybutu. Liczymy średnią temperaturę wewnątrz urządzenia w
podzbiorze przed wartością podziału. Od tej średniej odejmujemy wartość
w poszczególnych wierszach sumę tych różnic podniesionych do kwadratu
nazywamy SSR. Taką samą operacje wykonujemy na podzbiorze po wartości
podziału. Warunkiem podziału zostanie wartość w której suma SSR obu
zbiorów będzie najmniejsza. W węźle zostaje zapisany warunek podziału.
Operacje powtarza się na obu zbiorach uzyskanych takim podziałem.


\section{Przykład}
\label{\detokenize{dok_wstepna:przyklad}}

\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
czas(min)
&\sphinxstyletheadfamily 
alpha
&\sphinxstyletheadfamily 
temp
&\sphinxstyletheadfamily 
Ts
&\sphinxstyletheadfamily 
Ts+1
\\
\hline
0
&
1
&
6.50
&
2.00
&
2.18
\\
\hline
15
&
0
&
6.35
&
2.18
&
2.19
\\
\hline
30
&
0
&
6.20
&
2.19
&
1.68
\\
\hline
45
&
1
&
6.05
&
1.68
&
1.50
\\
\hline
60
&
1
&
5.90
&
1.50
&
1.64
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Czas w tym nie będzie potrzebny więc usunę go dla przejrzystości.
Załóżmy że ostatni wiersz to dane testowe. I rozmiar zbioru
niepodlegającego dzieleniu jest równy 2. Powiedzmy że będziemy dzielić
na podstawie temperatury zewnętrznej \sphinxstyleemphasis{‘temp’} sortujemy więc dane.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|}
\hline
\sphinxstyletheadfamily 
alpha
&\sphinxstyletheadfamily 
temp
&\sphinxstyletheadfamily 
Ts
&\sphinxstyletheadfamily 
Ts+1
\\
\hline
1
&
6.05
&
1.68
&
1.50
\\
\hline
0
&
6.20
&
2.19
&
1.68
\\
\hline
0
&
6.35
&
2.18
&
2.19
\\
\hline
1
&
6.50
&
2.00
&
2.18
\\
\hline
1
&
5.90
&
1.50
&
1.64
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}
\begin{description}
\item[{podział na \textgreater{}= 6.2}] \leavevmode
zbiór pierwszy {[}1.5{]} średnia = 1.5

zbiór drugi {[}1.68 2.19 2.18{]} średnia = \(\frac{1.68 + 2.19 + 2.18} {3} = 2.02\)

SSR = \((1.5 - 1.5)^2 + (1.68 - 2.02)^2 + (2.19 - 2.02)^2 + (2.18 - 2.02)^2 = 0.17\)

\item[{podział na \textgreater{}= 6.35}] \leavevmode
zbiór pierwszy {[}1.5 1.68{]} średnia = 1.59

zbiór drugi {[}2.19 2.18{]} średnia = 2.185

SSR = 0.016

\item[{podział na \textgreater{}= 6.5}] \leavevmode
zbiór pierwszy {[}1.5 1.68 2.19{]} średnia = 1.79

zbiór drugi {[}2.18{]} średnia = 2.18

SSR = 0.26

\end{description}

Podziału zbioru dokonujemy na ‘\sphinxstyleemphasis{temp}’ \textgreater{}= 6.35 ponieważ SSR jest wtedy
najmniejsze. Powstałe zbiory nie są większe od założonego rozmiaru
liścia więc kończymy budowę drzewa.

‘\sphinxstyleemphasis{temp}’ z danych testowych 5.9 \textless{} 6.35 więc przewidziana wartość to 1.59


\section{Tworzenie lasu losowego}
\label{\detokenize{dok_wstepna:tworzenie-lasu-losowego}}
Las losowy to zbiór drzew regresyjnych utworzonych na podstawie różnych
danych. Dane dla każdego drzewa są losowane z powtórzeniami.

Wynik przewidziany przez las jest średnią wyników ze wszystkich drzew.


\section{Eksperymenty}
\label{\detokenize{dok_wstepna:eksperymenty}}
W ramach eksperymentu można porównać działanie lasu losowego z metodą
naiwną (wartość z poprzedniego kwantu czasu). Można również sprawdzić
wpływ parametrów takich jaki ilość drzew, rozmiar liści, ilość danych
trenujących na działanie algorytmu.


\section{Zbiory Danych}
\label{\detokenize{dok_wstepna:zbiory-danych}}
Zbiór testujących będzie to ostatnie 10\% danych które nie będą użyte do
tworzenia drzew. Reszta danych to dane trenujące.

Jacek Dobrowolski
\phantomsection\label{\detokenize{tree:module-random_forest.tree}}\index{module@\spxentry{module}!random\_forest.tree@\spxentry{random\_forest.tree}}\index{random\_forest.tree@\spxentry{random\_forest.tree}!module@\spxentry{module}}

\chapter{Tree module}
\label{\detokenize{tree:tree-module}}\label{\detokenize{tree::doc}}
Module implementing regression tree
\index{Condition (class in random\_forest.tree)@\spxentry{Condition}\spxextra{class in random\_forest.tree}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Condition}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{random\_forest.tree.}}\sphinxbfcode{\sphinxupquote{Condition}}}{\emph{\DUrole{n}{index}}, \emph{\DUrole{n}{value}}}{}
Contains information about predictor index and treshold. Needed for branching
\index{index (random\_forest.tree.Condition attribute)@\spxentry{index}\spxextra{random\_forest.tree.Condition attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Condition.index}}\pysigline{\sphinxbfcode{\sphinxupquote{index}}}
Alias for field number 0

\end{fulllineitems}

\index{value (random\_forest.tree.Condition attribute)@\spxentry{value}\spxextra{random\_forest.tree.Condition attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Condition.value}}\pysigline{\sphinxbfcode{\sphinxupquote{value}}}
Alias for field number 1

\end{fulllineitems}


\end{fulllineitems}

\index{Config (class in random\_forest.tree)@\spxentry{Config}\spxextra{class in random\_forest.tree}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Config}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{random\_forest.tree.}}\sphinxbfcode{\sphinxupquote{Config}}}
Configuration for using tree module.
Overwrite them after importing module to configure tree building variables.
\index{min\_split\_size (random\_forest.tree.Config attribute)@\spxentry{min\_split\_size}\spxextra{random\_forest.tree.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Config.min_split_size}}\pysigline{\sphinxbfcode{\sphinxupquote{min\_split\_size}}}
Minimum size required for set of date to br further branched
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
int

\end{description}\end{quote}

\end{fulllineitems}

\index{number\_of\_predictors\_to\_draw (random\_forest.tree.Config attribute)@\spxentry{number\_of\_predictors\_to\_draw}\spxextra{random\_forest.tree.Config attribute}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Config.number_of_predictors_to_draw}}\pysigline{\sphinxbfcode{\sphinxupquote{number\_of\_predictors\_to\_draw}}}
Number of predictors which will be randomly drawn, from which the best split condition will be selected. It is recommended to use number higher than number of columns containing uniform data,
other wise best split may not be found.
\begin{quote}\begin{description}
\item[{Type}] \leavevmode
int

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Leaf (class in random\_forest.tree)@\spxentry{Leaf}\spxextra{class in random\_forest.tree}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Leaf}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{random\_forest.tree.}}\sphinxbfcode{\sphinxupquote{Leaf}}}{\emph{\DUrole{n}{training\_data}}}{}
Represent Leaf of the Tree. Contains possible values for prediction

\end{fulllineitems}

\index{Node (class in random\_forest.tree)@\spxentry{Node}\spxextra{class in random\_forest.tree}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Node}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{random\_forest.tree.}}\sphinxbfcode{\sphinxupquote{Node}}}{\emph{\DUrole{n}{training\_data}}}{}
Represents Node of the Tree. Contains {\hyperref[\detokenize{tree:random_forest.tree.Condition}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Condition}}}}} to select appropriate subnode ({\hyperref[\detokenize{tree:random_forest.tree.Node}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Node}}}}} or {\hyperref[\detokenize{tree:random_forest.tree.Leaf}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Leaf}}}}})
\index{find\_best\_split() (random\_forest.tree.Node method)@\spxentry{find\_best\_split()}\spxextra{random\_forest.tree.Node method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Node.find_best_split}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{find\_best\_split}}}{\emph{\DUrole{n}{training\_data}}}{}
Finds condition for split that gives lowest Residual Sum of Squares
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{training\_data}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} input data

\end{description}\end{quote}

\end{fulllineitems}

\index{rss() (random\_forest.tree.Node method)@\spxentry{rss()}\spxextra{random\_forest.tree.Node method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Node.rss}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{rss}}}{\emph{\DUrole{n}{array}}}{{ $\rightarrow$ float}}
Residual Sum of Squares
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{array}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} input array

\item[{Returns}] \leavevmode
Residual Sum of Squares

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}

\index{split\_data\_on\_value() (random\_forest.tree.Node method)@\spxentry{split\_data\_on\_value()}\spxextra{random\_forest.tree.Node method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Node.split_data_on_value}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{split\_data\_on\_value}}}{\emph{\DUrole{n}{data}}, \emph{\DUrole{n}{condition}\DUrole{p}{:} \DUrole{n}{{\hyperref[\detokenize{tree:random_forest.tree.Condition}]{\sphinxcrossref{random\_forest.tree.Condition}}}}}}{}
Splits np.ndarray using condition
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.array}}) \textendash{} data to be splitted

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{condition}} ({\hyperref[\detokenize{tree:random_forest.tree.Condition}]{\sphinxcrossref{\sphinxstyleliteralemphasis{\sphinxupquote{Condition}}}}}) \textendash{} condition of split

\end{itemize}

\item[{Returns}] \leavevmode
array lesser or equal to condition
right\_node(np.ndarray): array greater than condition

\item[{Return type}] \leavevmode
left\_node(np.ndarray)

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{Tree (class in random\_forest.tree)@\spxentry{Tree}\spxextra{class in random\_forest.tree}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Tree}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{random\_forest.tree.}}\sphinxbfcode{\sphinxupquote{Tree}}}{\emph{\DUrole{n}{training\_data}}}{}
Object representing regression tree
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{training\_data}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Data from which Tree will be created

\end{description}\end{quote}
\index{predict() (random\_forest.tree.Tree method)@\spxentry{predict()}\spxextra{random\_forest.tree.Tree method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.Tree.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{data}}}{{ $\rightarrow$ float}}
Calculates regression tree prediction
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} data for which prediction will be made.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{dimension array. Can be one field shorter than training data.}} (\sphinxstyleliteralemphasis{\sphinxupquote{Single}}) \textendash{} 

\end{itemize}

\item[{Returns}] \leavevmode
Prediction

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}

\index{create\_node() (in module random\_forest.tree)@\spxentry{create\_node()}\spxextra{in module random\_forest.tree}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{tree:random_forest.tree.create_node}}\pysiglinewithargsret{\sphinxcode{\sphinxupquote{random\_forest.tree.}}\sphinxbfcode{\sphinxupquote{create\_node}}}{\emph{\DUrole{n}{training\_data}}}{}
Calls appropriate constructor dependant on training\_data size.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{training\_data}} (\sphinxstyleliteralemphasis{\sphinxupquote{np.ndarray}}) \textendash{} Data from which node will be created

\item[{Returns}] \leavevmode
Node

\item[{Return type}] \leavevmode
{\hyperref[\detokenize{tree:random_forest.tree.Node}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Node}}}}}, {\hyperref[\detokenize{tree:random_forest.tree.Leaf}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{Leaf}}}}}

\end{description}\end{quote}

\end{fulllineitems}

\phantomsection\label{\detokenize{forest:module-random_forest.forest}}\index{module@\spxentry{module}!random\_forest.forest@\spxentry{random\_forest.forest}}\index{random\_forest.forest@\spxentry{random\_forest.forest}!module@\spxentry{module}}

\chapter{Forest module}
\label{\detokenize{forest:forest-module}}\label{\detokenize{forest::doc}}
Module implementing random regression forest
\index{Config (class in random\_forest.forest)@\spxentry{Config}\spxextra{class in random\_forest.forest}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{forest:random_forest.forest.Config}}\pysigline{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{random\_forest.forest.}}\sphinxbfcode{\sphinxupquote{Config}}}
Configuration Object, member fields must be overwritten using {\hyperref[\detokenize{forest:random_forest.forest.Config.create_config}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{create\_config()}}}}} after import.
\index{create\_config() (random\_forest.forest.Config method)@\spxentry{create\_config()}\spxextra{random\_forest.forest.Config method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{forest:random_forest.forest.Config.create_config}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{create\_config}}}{\emph{\DUrole{n}{number\_of\_trees}}, \emph{\DUrole{n}{data\_size\_for\_tree}}}{}
Configures forest
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode\begin{itemize}
\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{number\_of\_trees}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Number of trees for the forest to contain.

\item {} 
\sphinxstyleliteralstrong{\sphinxupquote{data\_size\_for\_tree}} (\sphinxstyleliteralemphasis{\sphinxupquote{int}}) \textendash{} Determines number of rows of training data

\end{itemize}

\end{description}\end{quote}

that go into every tree construction.

\end{fulllineitems}


\end{fulllineitems}

\index{Forest (class in random\_forest.forest)@\spxentry{Forest}\spxextra{class in random\_forest.forest}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{forest:random_forest.forest.Forest}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{class }}\sphinxcode{\sphinxupquote{random\_forest.forest.}}\sphinxbfcode{\sphinxupquote{Forest}}}{\emph{\DUrole{n}{data}}}{}
Represents random forest
\index{bootstrap() (random\_forest.forest.Forest method)@\spxentry{bootstrap()}\spxextra{random\_forest.forest.Forest method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{forest:random_forest.forest.Forest.bootstrap}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{bootstrap}}}{\emph{\DUrole{n}{data}}}{}
Randomly draws with repetitions rows form given \sphinxcode{\sphinxupquote{data}}.
Record are stored in list of \sphinxcode{\sphinxupquote{np.ndarrays}} form which trees are created.
Record that were not selected can by found in \sphinxcode{\sphinxupquote{self.out\_of\_bag}}.
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxcode{\sphinxupquote{pandas.DataFrame}}) \textendash{} training data for the forest

\item[{Returns}] \leavevmode
list of \sphinxcode{\sphinxupquote{np.ndarrays}} for each tree training

\item[{Return type}] \leavevmode
list

\end{description}\end{quote}

\end{fulllineitems}

\index{predict() (random\_forest.forest.Forest method)@\spxentry{predict()}\spxextra{random\_forest.forest.Forest method}}

\begin{fulllineitems}
\phantomsection\label{\detokenize{forest:random_forest.forest.Forest.predict}}\pysiglinewithargsret{\sphinxbfcode{\sphinxupquote{predict}}}{\emph{\DUrole{n}{data}}}{}
Calculates value predicted by the forest
\begin{quote}\begin{description}
\item[{Parameters}] \leavevmode
\sphinxstyleliteralstrong{\sphinxupquote{data}} (\sphinxcode{\sphinxupquote{np.ndarray}}) \textendash{} data for which prediction is made

\item[{Returns}] \leavevmode
average value of all trees predictions

\item[{Return type}] \leavevmode
float

\end{description}\end{quote}

\end{fulllineitems}


\end{fulllineitems}



\chapter{Dokumentacja Końcowa}
\label{\detokenize{main:dokumentacja-koncowa}}\label{\detokenize{main::doc}}
Projekt wykorzystuje algorytm lasu losowego drzew regresyjnych do predykcji szeregu czasowego.
Projekt ma na celu zapoznanie się z działaniem tego modelu i jego specyfiką.


\section{Przygotowanie danych}
\label{\detokenize{main:przygotowanie-danych}}
Aby drzewa miały większą ilość atrybutów do szeregu został dodany drugi szereg przesunięty o jeden kwant czasu.
Jako wartość do przewidzenia na koniec danych została dodana różnica temperatury z obecnej i przyszłej chwili.
Różnice temperatury mają znacznie mniejszy zakres wahań niż temperatury co umożliwia uzyskanie mniejszego błędu drzewom z mniejszą ilością liści.


\begin{savenotes}\sphinxattablestart
\centering
\begin{tabulary}{\linewidth}[t]{|T|T|T|T|T|T|T|T|}
\hline

time
&
compressor
&
ambient\_temp
&
refrigerator\_temp
&
compressor
&
ambient\_temp
&
refrigerator\_temp
&
refrigerator\_temp
\\
\hline
0
&
1
&
6.50
&
2.00
&
0.0
&
6.35
&
2.18
&
0.01
\\
\hline
15
&
0
&
6.35
&
2.18
&
0.0
&
6.20
&
2.19
&
\sphinxhyphen{}0.51
\\
\hline
30
&
0
&
6.20
&
2.19
&
1.0
&
6.05
&
1.68
&
\sphinxhyphen{}0.18
\\
\hline
45
&
1
&
6.05
&
1.68
&
1.0
&
5.90
&
1.50
&
0.14
\\
\hline
60
&
1
&
5.90
&
1.50
&
0.0
&
5.74
&
1.64
&
\sphinxhyphen{}0.02
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}

Dane testowe to 10\% danych losowo wybranych z całego zbioru.


\section{Implementacja}
\label{\detokenize{main:implementacja}}
Do implementacji użyłem języka python i bibliotek pandas i numpy.
Cały algorytm znajduje się w module \sphinxcode{\sphinxupquote{random\_forest}}.
Dzieli się on na dwa pliki jeden implementuje drzewa {\hyperref[\detokenize{tree:module-random_forest.tree}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{random\_forest.tree}}}}},
drugi natomiast las losowy {\hyperref[\detokenize{forest:module-random_forest.forest}]{\sphinxcrossref{\sphinxcode{\sphinxupquote{random\_forest.forest}}}}}.

Tworzenie lasu zaczyna się od losowania z powtórzeniami danych do budowy poszczególnych drzew.
Następnie budowane są drzewa na podstawie podanych danych i konfiguracji.
Budowa drzewa jest zaimplementowania rekurencyjnie. Tworzone są węzły i liście zależnie od rozmiaru zbioru danych.
Tworzenie liścia polega na wyliczeniu średniej wartości zmiennej do przewidywania.
tworzenie węzła polega na znalezieniu miejsca podziału które zminimalizuje funkcje kosztu.
W tym przypadku jest to suma kwadratów różnic. Kiedy zostanie wyznaczony najlepszy podział danego zbioru.
Tworzone są liście na podzbiorach.

Implementacja ma pewne ograniczenia. Zmienna dla której uczy się las musi być w ostatniej kolumnie.
Oraz minimalna ilość atrybutów losowanych do tworzenia węzłów musi być większa niż ilość atrybutów
gdzie mogą wystąpić takie same wartości (np. zmienna boolean) ponieważ może to spowodować że najlepszy
podział dla danego podzbioru nie istnieje. Innym problemem jest powolność częściowo spowodowana złożonością algorytmu,
aby zminimalizować ten problem drzewa budowane są w oddzielnych procesach co pozwala wykorzystać cały procesor.


\section{Eksperymenty}
\label{\detokenize{main:eksperymenty}}
Aby zmniejszyć wpływ losowości danych i budowy lasu każdy eksperyment będzie powtórzony 10 razy i wynik uśredniony.
Jako błąd użyty został średni błąd względny z wszystkich iteracji.
Metoda naiwna przewiduje temperaturę z poprzedniej chwili.
Las przewiduje zmianę temperatury następnie zostaje on zsumowana z obecną temperaturą.

Najpierw zbadam wpływ rozmiaru liścia na błąd na zbiorze testowym.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{plot1}.pdf}
\end{figure}

oraz na zbiorze trenującym.
Im mniejszy liść (więcej liści) tym mniejszy błąd.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{plot6}.pdf}
\end{figure}

Jak widać małe rozmiary liścia prowadzą do przetrenowania modelu (błąd na zbiorze trenującym jest mniejszy niż na testowym).

Wpływ ilości losowanych atrybutów

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{plot2}.pdf}
\end{figure}

Zbyt mała ilość atrybutów prowadzi do słabych podziałów w drzewach co prowadzi do większego błędu.
Natomiast z byt duża ilość prowadzi to do przetrenowania.

Ilość drzew

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{plot3}.pdf}
\end{figure}

Ilości drzew powyżej 50 nie poprawiają działania algorytmu natomiast znacznie zwalniają jego działanie

Ilość danych w drzewie

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{plot4}.pdf}
\end{figure}

Większa ilość danych użytych do budowy drzew poprawia wynik.

Wpływ ilości próbek z poprzednich chwili czasu.

\begin{figure}[htbp]
\centering

\noindent\sphinxincludegraphics{{plot5}.pdf}
\end{figure}

Użycie większej ilości próbek z poprzednich chwil czasu nie wiele pomaga modelowi.
Może być to spowodowane mniejszą korelacją dalej odległych próbek.


\section{Wnioski}
\label{\detokenize{main:wnioski}}
Las losowy nie sprawdza się najlepiej w tym zastosowaniu ze względu na obecność dość dobrej metody naiwnej.


\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\DUrole{xref,std,std-ref}{search}

\end{itemize}


\renewcommand{\indexname}{Python Module Index}
\begin{sphinxtheindex}
\let\bigletter\sphinxstyleindexlettergroup
\bigletter{r}
\item\relax\sphinxstyleindexentry{random\_forest.forest}\sphinxstyleindexpageref{forest:\detokenize{module-random_forest.forest}}
\item\relax\sphinxstyleindexentry{random\_forest.tree}\sphinxstyleindexpageref{tree:\detokenize{module-random_forest.tree}}
\end{sphinxtheindex}

\renewcommand{\indexname}{Index}
\printindex
\end{document}